{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "from scipy.optimize import minimize \n",
    "from multiprocessing import Pool, Process, Queue\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score as accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining loss function of logistic regression\n",
    "\n",
    "def log_logistic(X):\n",
    "    out = np.empty_like(X)\n",
    "    ind = X >0\n",
    "    out[ind] = -np.log(1.0+np.exp(-X[ind]))\n",
    "    out[~ind] = X[~ind] - np.log(1.0+np.exp(X[~ind]))\n",
    "    return out\n",
    "\n",
    "def loss_function(w,X,y,return_arr = None):\n",
    "    yz = y*np.dot(X,w)\n",
    "    if return_arr == True:\n",
    "        out = -(log_logistic(yz))\n",
    "    else:\n",
    "        out = -np.sum(log_logistic(yz))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x,y,x_sensitive,loss_function,sep_cons,gamma):\n",
    "    max_iter = 100000\n",
    "    contraint = []\n",
    "    #first train with no constraint to get L(theta*)\n",
    "    w = minimize(fun=loss_function,x0 = np.random.rand(x.shape[1],),args = (x,y),method = 'SLSQP', options = {'maxiter':max_iter}, constraints=[])\n",
    "    old_w = deepcopy(w.x)\n",
    "    unconstrained_loss_arr = loss_function(old_w,x,y,return_arr=True)\n",
    "\n",
    "    def constraint_gamma_all(w,x,y,initial_loss_arr):\n",
    "        gamma_arr = np.ones_like(y)*gamma\n",
    "        new_loss = loss_function(w,x,y)\n",
    "        old_loss = np.sum(initial_loss_arr)\n",
    "        return((1.0+gamma)*old_loss)-new_loss\n",
    "        \n",
    "    def constraint_protected_people(w,x,y):\n",
    "        #for fine-gamma, constraint to prevent non-protected user be classify as negative\n",
    "        return(np.dot(w,x.T))\n",
    "    \n",
    "    def constraint_unprotected_people(w,ind,old_loss,x,y):\n",
    "        new_loss = loss_function(w,np.array(x),np.array(y))\n",
    "        return((1.0+gamma)*old_loss)-new_loss\n",
    "\n",
    "    constraints = []\n",
    "    predicted_labels = np.sign(np.dot(w.x,x.T))\n",
    "\n",
    "    if sep_cons == False:\n",
    "        #gamma_LR\n",
    "        con = ({'type':'ineq','fun':constraint_gamma_all,'args':(x,y,unconstrained_loss_arr)})\n",
    "        constraints.append(con)\n",
    "    else:\n",
    "        #fine-gamma LR\n",
    "        for i in range(0,len(predicted_labels)):\n",
    "            if predicted_labels[i]==1.0 and x_sensitive[i] == 1.0:\n",
    "                con = ({'type':'ineq','fun':constraint_protected_people,'args':(x[i],y[i])})\n",
    "            else:\n",
    "                con = ({'type':'ineq','fun':constraint_unprotected_people,'args':(i,unconstrained_loss_arr[i],x[i],y[i])})\n",
    "            constraints.append(con)\n",
    "    \n",
    "    def opt_function(w,x,x_sensitive):\n",
    "        covariance = (x_sensitive - np.mean(x_sensitive))*np.dot(w,x.T)\n",
    "        return float(abs(sum(covariance)))/float(x.shape[0]) #equation2\n",
    "    \n",
    "    #train the gamma/fine-gamma lr according to the cross covariance\n",
    "    w = minimize(fun = opt_function,x0=old_w,args=(x,x_sensitive),method='SLSQP',options ={'maxiter':max_iter},constraints=constraints)\n",
    "\n",
    "    return w.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    z = np.dot(w,x.T)\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    y = (y>=0.5)\n",
    "    y = y.astype('float64')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation functions\n",
    "def get_calibration(y_pred, y_true,x_sensitive):\n",
    "    y_pred = y_pred.astype('float64')\n",
    "    y_true = y_true.astype('float64')\n",
    "    x_sensitive = x_sensitive.astype('float64')\n",
    "    idx = (x_sensitive == 1.0)\n",
    "    p1 = np.mean(y_pred[idx]==y_true[idx])\n",
    "    p0 = np.mean(y_pred[~idx]==y_true[~idx])\n",
    "    out = p1-p0\n",
    "    return out\n",
    "def evaluation(y_pred,y_true,x_sensitive):\n",
    "    cal = get_calibration(y_pred,y_true,x_sensitive)\n",
    "    acc  = accuracy_score(y_pred,y_true)\n",
    "    print(\"The accuracy score is: \",acc)\n",
    "    print('The calibration score is:',cal)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.5\n",
      "The calibration score is: 0.0\n"
     ]
    }
   ],
   "source": [
    "x =np.array([[1,2,3,5,2],[1,4,3,2,3],[1,6,3,4,2],[1,2,5,6,3]])\n",
    "y = np.array([0,1,1,0])\n",
    "x_sensitive = np.array([1,0,1,0])\n",
    "w = train_model(x,y,x_sensitive,loss_function,sep_cons=True,gamma=0.5)\n",
    "y_pred = predict(w,x)\n",
    "get_calibration(y_pred,y,x_sensitive)\n",
    "evaluation(y_pred,y,x_sensitive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
